{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### To convert data in VIA format to JSON format that the detection training code can read."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os, sys\n",
    "from typing import Any, Dict, List, Optional, Tuple, Union\n",
    "import hashlib\n",
    "import cv2\n",
    "import random\n",
    "random.seed(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Example format of the labels.json file:\n",
    "\n",
    "{\n",
    "    \"sample_img_01.png\" = {\n",
    "        'img_dimensions': (900, 600),\n",
    "        'img_hash': \"theimagedumpmyhash\",\n",
    "        'polygons': [[[x1, y1], [x2, y2], [x3, y3], [x4, y4]], ...]\n",
    "     },\n",
    "     \"sample_img_02.png\" = {\n",
    "        'img_dimensions': (900, 600),\n",
    "        'img_hash': \"thisisahash\",\n",
    "        'polygons': [[[x1, y1], [x2, y2], [x3, y3], [x4, y4]], ...]\n",
    "     }\n",
    "     ...\n",
    "}\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions\n",
    "\n",
    "# via2df on ocr tokens\n",
    "def via2df(via_tokens, exclude_list=[]):\n",
    "    doc_ids, page_ids,page_nums, xs, ys, widths, heights, texts = [], [], [], [], [], [], [], []\n",
    "\n",
    "    for key, value in tqdm(via_tokens['_via_img_metadata'].items()):\n",
    "        page_id = value['filename']\n",
    "        doc_id = page_id.rsplit('+', maxsplit=1)[0]\n",
    "        page_num = page_id.rsplit('+', maxsplit=1)[1][:-4]\n",
    "        page_id = doc_id + '+' + page_num\n",
    "        if doc_id not in exclude_list:\n",
    "            \n",
    "            regions = value['regions']\n",
    "            \n",
    "\n",
    "            for region in regions:\n",
    "                x = region['shape_attributes']['x']\n",
    "                y = region['shape_attributes']['y']\n",
    "                width = region['shape_attributes']['width']\n",
    "                height = region['shape_attributes']['height']\n",
    "                text = region['region_attributes']['text']\n",
    "\n",
    "                doc_ids.append(doc_id)\n",
    "                page_ids.append(page_id)\n",
    "                page_nums.append(page_num)\n",
    "                xs.append(x)\n",
    "                ys.append(y)\n",
    "                widths.append(width)\n",
    "                heights.append(height)\n",
    "                texts.append(text)\n",
    "    \n",
    "    data_tuple = list(zip(doc_ids,page_ids,page_nums,xs,ys,widths,heights,texts))\n",
    "    token_df = pd.DataFrame(data_tuple, columns=['doc_ids','page_ids','page_num','x','y','width','height','text'])\n",
    "    return token_df\n",
    "\n",
    "\n",
    "# standardize datatypes\n",
    "convert_dict = {'x':int,\n",
    "            'y':int,\n",
    "            'width':int,\n",
    "            'height':int,\n",
    "            'page_num':str}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input and output files and directories\n",
    "\n",
    "# TODO: change to the path to the via file\n",
    "via_dir = '/home/mzhao/Data/work/DocAI/src/cba/data/us_dl/output_manually_annotated.json'\n",
    "\n",
    "with open(via_dir) as data_file:\n",
    "    via_file = json.load(data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input image file directory\n",
    "# TODO: change to the path to the image file directory\n",
    "img_dir = '/home/mzhao/Data/work/DocAI/src/cba/data/us_dl/images'\n",
    "# output file directory for the json file\n",
    "label_dir = '/home/mzhao/Data/work/DocAI/src/cba/data/us_dl/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [00:00<00:00, 38681.42it/s]\n"
     ]
    }
   ],
   "source": [
    "# convert via to dataframe\n",
    "ocr_df = via2df(via_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the document list from the annotation file\n",
    "doc_list = ocr_df.page_ids.unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train valid split\n",
    "train_valid_split = 0.9\n",
    "random.shuffle(doc_list)\n",
    "train_list = doc_list[:int(len(doc_list)*train_valid_split)]\n",
    "valid_list = doc_list[int(len(doc_list)*train_valid_split):]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# format the label file\n",
    "\n",
    "def convert_labels(img_dir, doc_list, ocr_df, output_dir, label_type ='train'):\n",
    "\n",
    "    labels: Dict[str, Any] ={}\n",
    "\n",
    "    for filename in doc_list:\n",
    "        attr: Dict[str, Any]={}\n",
    "\n",
    "        # get img_dimensions\n",
    "        img_path = os.path.join(img_dir,filename+'.png')\n",
    "        img = cv2.imread(img_path)\n",
    "        attr['img_dimensions'] = img.shape[:2]\n",
    "        \n",
    "        # get img_hash\n",
    "        with open(img_path,'rb') as f:\n",
    "            bytes = f.read()\n",
    "            readable_hash = hashlib.sha256(bytes).hexdigest()\n",
    "        attr['img_hash'] = readable_hash\n",
    "\n",
    "        box_targets = ocr_df[ocr_df.page_ids == filename][['x','y','width','height']].values.tolist()\n",
    "        box_targets = [\n",
    "        [\n",
    "        [box[0], box[1]],\n",
    "        [box[0]+box[2], box[1]],\n",
    "        [box[0]+box[2], box[1]+box[3]],\n",
    "        [box[0], box[1]+box[3]],\n",
    "        ] for box in box_targets\n",
    "        ]\n",
    "        attr['polygons'] = box_targets\n",
    "\n",
    "        labels[filename+'.png'] = attr\n",
    "        \n",
    "    # print 1 example of the label file\n",
    "    print(labels[doc_list[0]+'.png'])\n",
    "    \n",
    "    # dump the json file\n",
    "    with open(os.path.join(output_dir, f'{label_type}_labels.json'),'w') as json_file:\n",
    "        json.dump(labels, json_file)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'img_dimensions': (1200, 1600), 'img_hash': 'ba9548eed30e45c5321b5098c5480bdf5f3b702439e35a2ef90262b33bacb4f5', 'polygons': [[[802, 290], [1119, 290], [1119, 351], [802, 351]], [[132, 550], [217, 550], [217, 591], [132, 591]], [[185, 599], [437, 599], [437, 645], [185, 645]], [[404, 698], [468, 698], [468, 740], [404, 740]], [[114, 789], [195, 789], [195, 847], [114, 847]], [[209, 790], [318, 790], [318, 844], [209, 844]], [[336, 791], [473, 791], [473, 839], [336, 839]], [[627, 795], [734, 795], [734, 841], [627, 841]], [[751, 796], [881, 796], [881, 842], [751, 842]], [[204, 885], [302, 885], [302, 943], [204, 943]], [[311, 889], [448, 889], [448, 936], [311, 936]], [[579, 891], [678, 891], [678, 937], [579, 937]], [[438, 279], [784, 279], [784, 347], [438, 347]], [[516, 346], [757, 346], [757, 389], [516, 389]], [[766, 350], [1053, 350], [1053, 394], [766, 394]], [[1111, 395], [1267, 395], [1267, 435], [1111, 435]], [[1261, 393], [1321, 393], [1321, 437], [1261, 437]], [[1101, 431], [1253, 431], [1253, 454], [1101, 454]], [[1115, 464], [1244, 464], [1244, 493], [1115, 493]], [[1205, 499], [1376, 499], [1376, 547], [1205, 547]], [[1248, 460], [1373, 460], [1373, 497], [1248, 497]], [[1096, 938], [1321, 938], [1321, 992], [1096, 992]], [[570, 851], [695, 851], [695, 895], [570, 895]], [[572, 791], [620, 791], [620, 845], [572, 845]], [[580, 751], [672, 751], [672, 797], [580, 797]], [[685, 743], [722, 743], [722, 789], [685, 789]], [[734, 751], [830, 751], [830, 797], [734, 797]], [[485, 703], [585, 703], [585, 738], [485, 738]], [[131, 697], [389, 697], [389, 730], [131, 730]], [[119, 643], [192, 643], [192, 691], [119, 691]], [[202, 655], [452, 655], [452, 690], [202, 690]], [[447, 657], [516, 657], [516, 692], [447, 692]], [[256, 745], [356, 745], [356, 787], [256, 787]], [[360, 847], [416, 847], [416, 887], [360, 887]], [[110, 878], [206, 878], [206, 940], [110, 940]], [[119, 741], [257, 741], [257, 785], [119, 785]], [[99, 840], [356, 840], [356, 882], [99, 882]]]}\n",
      "{'img_dimensions': (450, 600), 'img_hash': '7601c0d4cc39aee7ef8684405a21de00f8a86f3b4ea0f8f26d47bb27d9573b10', 'polygons': [[[50, 144], [147, 144], [147, 163], [50, 163]], [[49, 163], [70, 163], [70, 182], [49, 182]], [[97, 163], [109, 163], [109, 183], [97, 183]], [[48, 163], [171, 163], [171, 183], [48, 183]], [[174, 165], [197, 165], [197, 183], [174, 183]], [[47, 203], [139, 203], [139, 223], [47, 223]], [[144, 205], [175, 205], [175, 223], [144, 223]], [[179, 205], [217, 205], [217, 223], [179, 223]], [[239, 248], [358, 248], [358, 264], [239, 264]], [[51, 267], [202, 267], [202, 288], [51, 288]], [[43, 298], [119, 298], [119, 314], [43, 314]], [[123, 299], [168, 299], [168, 315], [123, 315]], [[235, 297], [340, 297], [340, 314], [235, 314]], [[237, 315], [252, 315], [252, 335], [237, 335]], [[42, 357], [105, 357], [105, 374], [42, 374]], [[108, 358], [154, 358], [154, 374], [108, 374]], [[416, 368], [556, 368], [556, 396], [416, 396]], [[414, 368], [555, 368], [555, 396], [414, 396]], [[79, 73], [296, 73], [296, 108], [79, 108]], [[303, 87], [407, 87], [407, 108], [303, 108]], [[405, 89], [528, 89], [528, 106], [405, 106]], [[465, 133], [532, 133], [532, 145], [465, 145]], [[533, 132], [564, 132], [564, 145], [533, 145]], [[475, 145], [568, 145], [568, 169], [475, 169]], [[423, 336], [555, 336], [555, 360], [423, 360]], [[44, 244], [121, 244], [121, 263], [44, 263]], [[244, 263], [392, 263], [392, 288], [244, 288]], [[201, 261], [236, 261], [236, 289], [201, 289]], [[296, 105], [433, 105], [433, 126], [296, 126]], [[176, 104], [292, 104], [292, 119], [176, 119]], [[92, 123], [111, 123], [111, 142], [92, 142]], [[31, 15], [546, 15], [546, 58], [31, 58]], [[44, 122], [92, 122], [92, 142], [44, 142]], [[128, 243], [191, 243], [191, 260], [128, 260]], [[41, 374], [145, 374], [145, 407], [41, 407]], [[37, 315], [93, 315], [93, 345], [37, 345]]]}\n"
     ]
    }
   ],
   "source": [
    "# get the labels\n",
    "convert_labels(img_dir, train_list, ocr_df, label_dir, label_type='train')\n",
    "convert_labels(img_dir, valid_list, ocr_df, label_dir, label_type='valid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('DocAI')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cd84a7078e5d49b8c733fc74460d5cb383fdf0c65e5569ac2bddf9d35a624ffc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
